# Testy statystyczne

```{=latex}
\fancyhead[LO]{\textbf{\small{Testy statystyczne}}}
\renewcommand{\headrulewidth}{0.5pt}
```

## Test asymptotyczny

Niech $L^2[a,b], a,b\in\doubleR, a<b$ oznacza przestrzeń Hilberta funkcji całkowalnych z kwadratem określonych na przedziale $[a,b]$, tj. $$f\in L^2[a,b] \wtw \int_a^bf^2(x)\,dx<\oo.$$

\AAA{do dopisania: def. wartości własnych funkcji, centralny rozkład $\chi^2$}

W podejściu asymptotycznym przedstawionym w pracy \cite{MCC} formułuje się następujące twierdzenie:
\begin{tw}\label{tw:asympt}
Niech $$X_i(t) = m(t)+\epsilon_i(t), 1 \le i \le n$$ będą $n$ niezależnymi procesami losowymi należącymi do przestrzeni $L^2[0,2]$ definiowanymi na przedziale $[0,2]$, gdzie $m$ jest daną funkcją, a $\epsilon_i$ jest procesem losowym o wartości oczekiwanej równej zero oraz funkcji kowariancji $\doubleC(s,t)$.

Załóżmy, że:
\begin{enumerate}
  \item $\tr(\doubleC) \coloneqq \int_0^2\doubleC(t,t)\,dt < \oo $
  \item $v_1(t) \coloneqq X_1(t) - m(t) $ spełnia własność
  \[
  \doubleE\norm{v_1}^4 = \doubleE \nz{\int_0^2 v^2_1(t)\,dt}^2 <\oo
  \]
  \item $\bigwedge_{t\in[0,2]}\nz{ \doubleC(t,t) >0\quad\land\quad \max_{t\in [0,2]} \doubleC(t,t) <\oo} $
  \item $\bigwedge_{(s,t)\in [0,2]^2} \doubleE(v_1^2(s)v_1^2(t)) < C < \oo $, gdzie $C$ jest pewną stałą niezależną od $(s,t)$
\end{enumerate}
\AAA{czy te założenia są tu wykorzystywane? bo mam wrażenie, że nie} \textcolor{blue}{Faktycznie nie wszystkie są potrzebne do tego twierdzenia. Tu potrzebne są A1 i A3. Proszę uzupełnić poniższy dowód z wykorzystaniem tego co jest w moim artykule na stronach 528-529. W samym twierdzeniu najlepiej zrobić tak: Przed twierdzeniem podać powyższe założenia, tylko napisać, że będziemy używać poniższych założeń w zależności od potrzeb. Natomiast w tym twierdzeniu napisać ,,Przyjmy, że prawdziwe są założenia A1 i A3.}

Wówczas przy prawdziwości hipotezy zerowej $H_0: \bigwedge_{t\in[0,1]}m(t)=m(1+t)$ mamy
\[
\scriptC_n \xto{d} \scriptC^* \coloneqq \sum_{k\in\doubleN} \lambda_kA_k,
\]
gdzie $A_k$ są niezależnymi zmiennymi losowymi o centralnym rozkładzie $\chi^2(1)$, a $\lambda_1,\lambda_2,\dots$ są wartościami własnymi funkcji kowariancji $$\doubleK(s,t)=\doubleC(s,t)-\doubleC(s,t+1)-\doubleC(s+1,t)+\doubleC(s+1,t+1),\ s,t\in[0,1]$$ takimi, że $\lambda_1\ge \lambda_2\ge \dots\ge 0$ i $\sum_{i=1}^\infty\lambda_i^2<\infty$.

\AAA{czy to "\underline{centralnym} rozkładzie chi2" jest konieczne?} \textcolor{blue}{nie jest konieczne, ale dobrze, aby Pan wiedział, że istnieje niecentralny rozkład chi-kwadrat}
\end{tw}
\begin{proof}
Korzystając z {Centralnego Twierdzenia Granicznego dla} zmiennych losowych przyjmujących wartości w przestrzeni Hilberta $L^2[0,2]$ z miarą probabilistyczną, zachodzi zbieżność
\[
\sqrt{n}(\bar{X}(t) -m(t)) \xto{d} z(t)
\]
gdzie $z$ jest procesem gaussowskim o wartości oczekiwanej 0 i funkcji kowariancji $\doubleC(s,t)$.
\AAA{czy jeśli w dowodzie jest twierdzenie, to mam je dopisać(+dowieść), czy wystarczy mi wiedzieć, jak działa?} \textcolor{blue}{można dopisać}
Przy założeniu hipotezy zerowej, z {twierdzenia o odwzorowaniach ciągłych}
\begin{equation}\label{eqn:zb.as.cn}
\scriptC_n = n \int_0^1 (\bar{X}(t) - \bar{X}(t+1))^2\,dt \xto{d} \int_0^1 (z(t)-z(1+t))^2\,dt
\end{equation}
Oznaczając $\xi(t) = z(t)-z(1+t)$ można zaobserwować, że również jest to proces gaussowski o wartości oczekiwanej 0. Funkcja kowariancji tego procesu:
\textcolor{blue}{
\begin{align*}
\doubleK(s,t)=&\Cov(\xi(s),\xi(t)\\
=&\doubleE\nk{\nz{\xi(s)-\doubleE(\xi(s)}\nz{\xi(t)-\doubleE(\xi(t))}}\\
=&\doubleE\nk{\nz{z(s)-z(1+s)-\doubleE(z(s)-z(1+s))}\nz{z(t)-z(1+t)-\doubleE(z(t)-z(1+t))}}\\
=&\doubleE\nk{\nz{z(s)-\doubleE(z(s))-(z(1+s)-\doubleE(z(1+s)))}\nz{z(t)-\doubleE(z(t))-(z(1+t)-\doubleE(z(1+t)))}}\\
=&\doubleE\nk{\nz{z(s)-\doubleE(z(s))}\nz{z(t)-\doubleE(z(t))}}\\
&-\doubleE\nk{\nz{z(s)-\doubleE(z(s))}\nz{(z(1+t)-\doubleE(z(1+t)))}}\\
&-\doubleE\nk{\nz{(z(1+s)-\doubleE(z(1+s)))}\nz{z(t)-\doubleE(z(t))}}\\
&+\doubleE\nk{\nz{z(1+s)-\doubleE(z(1+s))}\nz{z(1+t)-\doubleE(z(1+t))}}\\
=&\doubleC(s,t) - \doubleC(s,t+1) - \doubleC(s+1,t) + \doubleC(s+1,t+1).
\end{align*}}
jest funkcją z przestrzeni Hilberta $L^2[0,1]$. Wobec tego można zastosować rozkład \AAA{Karhunena-Loeve'a?}, co daje równość
\begin{equation}\label{eqn:rozkladKL}
\int_0^1 \xi(t)^2\,dt = \sum_{k\in\doubleN} \lambda_kA_k^2
\end{equation}
\end{proof}

\AAA{p-wartość tego? niby jest wzór w \cite{MCC}, ale ona jest wyznaczana symulacyjnie? trzeba opisać kroki algorytmu znowu?} \textcolor{blue}{rozpisujemy ten test, ponieważ jest to test asymptotyczny (test A)}

## Testy bootstrapowe i permutacyjne

Założenie addytywności przedstawione w równaniu \eqref{eqn:addytywnosc} nie jest spełnione dla wielu różnych funkcji, w szczególności funkcji gęstości -- muszą one spełniać konkretne warunki (m.in. $f\ge0$ oraz $\int_\doubleR f = 1$). W związku z tym w pracy \cite{MCC} przedstawia się dwa podejścia nieparametryczne, niewymagające wyżej wspomnianego założenia o addytywności.

Przy prawdziwości hipotezy zerowej spełniona jest równość
\begin{equation}\label{eqn:c_h0}
\scriptC_n = n\int_0^1 (\bar{X}(t) - m(t) + m(1+t) - \bar{X}(1+t))^2 dt.
\end{equation}
W celu przybliżenia rozkładu prawdopodobieństwa powyższej statystyki można wykorzystać **metodę bootstrapową**. \AAA{definicja bootstrapu}. Metoda ta przebiega w następujących krokach:

1. Obliczyć wartość statystyki $\scriptC_n$ na podstawie danych oryginalnych. Niech $\scriptC_0$ oznacza tę wartość.
2. Wybrać $B$ niezależnych prób bootstrapowych $\mathbf{X}^*_i, i = 1,\ldots,B$. Każda z nich składa się z $n$ funkcji:
\[
\mathbf{X}^*_b = \set{X_1^{*,b}(t),\ldots, X_n^{*,b}(t)}, \quad t\in[0,2], b = 1,\ldots,B
\]
3. Dla każdej próby bootstrapowej wybranej w kroku 2. obliczyć wartości statystyki testowej $\scriptC_n$. Niech $\scriptC_1,\scriptC_2,\dots,\scriptC_B$ oznaczają otrzymane wartości.
4. Obliczyć $p$-wartość według wzoru: $$\frac{1}{B}\sum_{i=1}^BI(\scriptC_i>\scriptC_0).$$
```{=latex}
% różnice względem oznaczeń w MCC:
% MCC         | ja
% -------------------------------
% Cn          | C0
% X^*_b       | X^*_b -- tak samo
% C^{*,b}_n   | C_b
```

```{=latex}
\begin{tw}
Jeżeli $\scriptC^*$ jest statystyką testową obliczoną na podstawie powyższego algorytmu, to
\begin{equation}\label{eqn:bootstrapstat}
\scriptC^*_n = n\int_0^1 \nz{\bar{X^*}(t)- \bar{X}(t) + \bar{X}(t+1) - \bar{X^*}(t+1)}^2 \,dt \xto{d} \int_0^1 \nz{z^*(t) - z^*(t+1)}^2 \,dt
\end{equation}
gdzie $z^*(t) - z^*(t+1)$ jest procesem gaussowskim o wartości oczekiwanej równej 0 i funkcji kowariancji $$\doubleK^*(s,t) = \doubleS(s,t) - \doubleS(s, t+1) - \doubleS(s+1,t) + \doubleS(s+1,t+1)  $$
gdzie $\doubleS(s,t), s,t\in[0,2]$ jest funkcją kowariancji z próby, tj. funkcją kowariancji $\bar{X}$.
\end{tw}
```
\begin{proof}
\AAA{no właśnie, \cite{MCC} twierdzi chyba, że dowód wymaga dogłębnych badań?} \textcolor{blue}{powyższe twierdzenie bym usunął i podał jego treść w luźnej formie jak to robią MCC. Aby to pokazać to trzeba więcej wysiłku, a to co oni podali to tylko intuicja.}
\end{proof}

\AAA{This resample algorithm takes advantage of the particular estimator properties and it is generalizable in order to approximate the distribution of general statistics -- jakie general statistics?} \textcolor{blue}{tutaj general znaczy dowolnej statystyki} Najbardziej powszechną do tego celu metodą jest **test permutacyjny**, który zakłada, że zależność każdej badanej pary danych jest taka sama \AAA{?} \textcolor{blue}{tzn. że rodzaj zależności dla każdej pary jest taki sam, np. liniowy}. Założenie to zwykle nie działa dla większej liczby prób \cite{MCC}.

Przy zastosowaniu techniki permutacyjnej do prowadzonych rozważań otrzymuje się następujący algorytm, będący nieznaczną modyfikacją algorytmu bootstrapowego:

1. Obliczyć wartość statystyki $\scriptC_n$ na podstawie danych oryginalnych. Niech $\scriptC_0$ oznacza tę wartość.
2. Wybrać $B$ niezależnych prób *permutacyjnych* $\mathbf{X}^*_i, i = 1,\ldots,B$. Każda z nich składa się z $n$ funkcji:
\[
\mathbf{X}^*_b = \set{X_1^{*,b}(t),\ldots, X_n^{*,b}(t)}, \quad t\in[0,2], b = 1,\ldots,B
\]
gdzie z prawdopodobieństwem równym $\frac{1}{2}$ dla $i=1,\ldots,n$:
\[
X_i^{*,b}(t) = X_i(t) \qquad\lor\qquad
X_i^{*,b}(t) = X_i(t + (-1)^{I_{[1,2]}(t)})
\]
\AAA{Czy to nie znaczy, że po prawej może być $t-1$ i wyjeżdża się poza $[0,2]$? jaka jest idea tych permutacji, co permutujemy?} \textcolor{blue}{Nie tak być nie może: $X_i^{*,b}(t) = X_i(t + (-1)^{I_{[1,2]}(t)})$ oznacza, że gdy $t\in [0,1]$, to $X_i^{*,b}(t) = X_i(t + (-1)^{I_{[1,2]}(t)})=X_i(t + (-1)^0)=X_i(t + 1)$, a gdy $t\in[1,2]$, to $X_i^{*,b}(t) = X_i(t + (-1)^{I_{[1,2]}(t)})=X_i(t + (-1)^1) = X_i(t -1)$. Chodzi o to, że albo zostawiamy dane tak jak są ($X_i^{*,b}(t) = X_i(t)$) lub zamieniamy dane przed (dla $t\in[0,1]$) z danymi po (dla $t\in[1,2]$) tj. $X_i^{*,b}(t) = X_i(t + (-1)^{I_{[1,2]}(t)})$.}
3. Dla każdej próby permutacyjnej wybranej w kroku 2. obliczyć wartości statystyki testowej $\scriptC_n$. Niech $\scriptC_1,\scriptC_2,\dots,\scriptC_B$ oznaczają otrzymane wartości.
4. Obliczyć $p$-wartość według wzoru: $$\frac{1}{B}\sum_{i=1}^BI(\scriptC_i>\scriptC_0).$$

\begin{tw}
\AAA{nie mogę znaleźć zbieżności tegoż} \textcolor{blue}{bez twierdzenia z podobnych powodów jak powyżej dla bootstrapu}
\end{tw}

## Test oparty o przybliżenie Boxa

W \cite{Smaga} zwrócono uwagę na problem wyżej opisanych metod, jakim jest ich czasochłonność. W celu znalezienia szybszego sposobu przybliżenia rozkładu statystyki $\scriptC_n$, w tej pracy wykorzystano przybliżenie Boxa przy założeniu hipotezy zerowej.

Wykorzystane zostaną założenia i wyniki Twierdzenia \ref{tw:asympt}. Rozkład statystyki $\scriptC_n$ jest znany, z wyjątkiem wartości własnych $\lambda_i, i\in\doubleN$ funkcji kowariancji $\doubleK(s,t)$. Te wartości własne można wyestymować, korzystając z estymatora
\begin{equation}\label{eqn:kkestimator}
\hat{\doubleK}(s,t) = \hat{\doubleC}(s,t) - \hat{\doubleC}(s, t+1) - \hat{\doubleC}(s-1, t) + \hat{\doubleC}(s+1, t+1), \quad s,t\in[0,1]
\end{equation}
gdzie $$\hat{\doubleC}(s,t) = \frac{1}{n-1}\sum_{i=1}^n (X_i(s) - \bar{X}(s))(X_i(t) - \bar{X}(t)), s,t\in [0,2] $$ jest nieobciążonym estymatorem funkcji kowariancji $\doubleC(s,t)$ \cite{Zhang}.

\begin{lm}
Przyjmując założenia Twierdzenia \ref{tw:asympt} oraz założenia A1-A4 zachodzi jednostajna zbieżność wg prawdopodobieństwa $$\hat{\doubleK}(s,t) \xto{P} \doubleK(s,t)$$ na $[0,1]^2$.
\end{lm}
\begin{proof}
Na podstawie wykorzystywanych założeń zachodzi $\hat{\doubleC}(s,t)\xto{P} \doubleC(s,t)$ jednostajnie na $[0,2]^2$ \textcolor{blue}{(patrz Twierdzenie 4.17 w \cite{Zhang})}. Korzystając z definicji estymatora podanej w \eqref{eqn:kkestimator} oraz z {twierdzenia o odwzorowaniu ciągłym}, otrzymujemy tezę.
\end{proof}

Do oszacowania rozkładu statystyki $\scriptC_n$ przy założeniu hipotezy zerowej wykorzystane teraz zostanie **przybliżenie Boxa**. Metoda ta jest również znana jako *przybliżenie dwóch kumulant* \cite{Zhang}. Do oszacowań wykorzystywane są w niej *kumulanty*, często stosowane w analizie danych funkcjonalnych \cite{Smaga}.
\begin{defi}
Niech $X$ będzie zmienną losową, a $\psi_X(t)$ jej funkcją charakterystyczną. Jeżeli
$$
\log(\psi_X(t)) = \sum_{k=1}^\oo \kappa_k(X) \frac{it^k}{k!}
$$
to wielkości $\kappa_k(X)$ nazywane są \textbf{kumulantami} zmiennej losowej $X$.
\end{defi}
W szczególności pierwszymi 4 kumulantami są
\begin{align*}
\kappa_1(X) &= \doubleE(X), &\quad
\kappa_2(X) &= \Var(X),\\
\kappa_3(X) &= \doubleE(X-\doubleE(X))^3, &\quad
\kappa_4(X) &= \doubleE(X-\doubleE(X))^4 - 3\Var^2(X).
\end{align*}

Główną ideą tejże metody jest oszacowanie rozkładu $\scriptC^*_0 \coloneqq \sum_{k\in\doubleN}\lambda_kA_k$ za pomocą rozkładu zmiennej losowej postaci
\[ \beta \chi^2(d), \]
gdzie parametry $\beta, d$ obliczane na podstawie przyrównania pierwszych dwóch kumulant zmiennych losowych $\scriptC_0^*$ oraz $\beta \chi^2(d)$. Wykorzystując obliczenia wykonane w pracy \cite{Zhang}, otrzymuje się
\begin{equation}\label{eqn:betad}
\beta = \frac{\tr(\doubleK^{\otimes 2})}{\tr(\doubleK)}, \quad
d = \frac{\tr^2(\doubleK)}{\tr(\doubleK^{\otimes 2})},
\end{equation}
gdzie
\[
\tr(\doubleK) = \int_0^1 \doubleK(t,t)\,dt, \quad
\doubleK^{\otimes 2} \coloneqq \int_0^1 \doubleK(s,u)\doubleK(u,t)\,du.
\]

Naturalnymi estymatorami parametrów ze wzoru \eqref{eqn:betad} są te uzyskane poprzez podstawienie estymatora funkcji kowariancji $\hat{\doubleK}(s,t)$ zdefiniowanego wzorem \eqref{eqn:kkestimator}, co prowadzi do wzorów

\begin{equation}\label{eqn:betadestimator}
\hat\beta = \frac{\tr(\hat\doubleK^{\otimes 2})}{\tr(\hat\doubleK)}, \quad
\hat{d} = \frac{\tr^2(\hat\doubleK)}{\tr(\hat\doubleK^{\otimes 2})}.
\end{equation}

Przy założeniu hipotezy zerowej otrzymuje się wówczas przybliżenie $\scriptC_n \sim \hat\beta \chi^2(\hat d)$, co prowadzi do testu, w którym $p$-wartość oblicza się wzorem
$$ \doubleP\nz{\chi^2(\hat d) > \frac{\scriptC_n}{\hat\beta}}.$$

\AAA{czy tyle wystarczy?} \textcolor{blue}{póki co tak, ale jeśli starczy czasu i miejsca, to coś dopiszemy}
